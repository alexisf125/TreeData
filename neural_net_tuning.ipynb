{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all files in \"data/good_data\" and concatenta them into one dataframe\n",
    "df = pd.concat([pd.read_csv(f\"data/good_data/{file}\") for file in os.listdir(\"data/good_data\")])\n",
    "# keep only common_name, condition, latitude_coordinate, longitude_coordinate, and native columns\n",
    "df = df[['common_name', 'condition', 'latitude_coordinate', 'longitude_coordinate', 'native']]\n",
    "# convert condition to numerical\n",
    "df['condition'] = df['condition'].replace({'excellent': 4, 'good': 3, 'fair': 2, 'poor': 1, 'dead/dying': 0, 'dead': 0})\n",
    "# one hot \"common_name\" column\n",
    "df = pd.get_dummies(df, columns=[\"common_name\"])\n",
    "# one hot native column\n",
    "df = pd.get_dummies(df, columns=[\"native\"])\n",
    "# drop native_no_info column\n",
    "df.drop(columns=['native_no_info'], inplace=True)\n",
    "# drop rows where condition is null\n",
    "df = df.dropna(subset=['condition'])\n",
    "\n",
    "\n",
    "# split into X and y\n",
    "X = df.drop('condition', axis=1)\n",
    "y = df['condition']\n",
    "\n",
    "# convert from boolean to int\n",
    "y = y.astype(int)\n",
    "X = X.astype(float)\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert data to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "X_test = torch.from_numpy(X_test.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "y_test = torch.from_numpy(y_test.values).long()\n",
    "\n",
    "# parameters\n",
    "input_size = len(X.columns)\n",
    "num_classes = len(df['condition'].unique())\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for hyperparameters\n",
    "for learning_rate in [0.0001, 0.001, 0.01, 0.1]:\n",
    "    for batch_size in [100, 200, 300, 400, 500]:\n",
    "        for hidden_size in [500, 1000, 1500]:\n",
    "            # create model\n",
    "            model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "            # loss and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # train model\n",
    "            total_step = len(X_train)\n",
    "            loss_list = []\n",
    "            acc_list = []\n",
    "            for epoch in range(num_epochs):\n",
    "                for i in range(0, total_step, batch_size):\n",
    "                    # get batch\n",
    "                    X_batch = X_train[i:i+batch_size]\n",
    "                    y_batch = y_train[i:i+batch_size]\n",
    "                    \n",
    "                    # forward pass\n",
    "                    outputs = model(X_batch)\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                    loss_list.append(loss.item())\n",
    "                    \n",
    "                    # backward and optimize\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # accuracy\n",
    "                    total = y_batch.size(0)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    correct = (predicted == y_batch).sum().item()\n",
    "                    acc_list.append(correct / total)\n",
    "                    \n",
    "                if (epoch+1) % 10 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {correct / total:.4f}')\n",
    "            print(f'learning_rate: {learning_rate}, batch_size: {batch_size}, hidden_size: {hidden_size}, accuracy: {correct / total:.4f}')\n",
    "\n",
    "# test model\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for i in range(len(X_test)):\n",
    "        X = X_test[i].unsqueeze(0)\n",
    "        y = y_test[i]\n",
    "        outputs = model(X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += y.size(0)\n",
    "        n_correct += (predicted == y).sum().item()\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the test images: {acc} %')\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for i in range(len(X_test)):\n",
    "        X = X_test[i].unsqueeze(0)\n",
    "        y = y_test[i]\n",
    "        outputs = model(X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += y.size(0)\n",
    "        n_correct += (predicted == y).sum().item()\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the test data: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy and loss\n",
    "plt.plot(loss_list)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.plot(acc_list)\n",
    "plt.title('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
